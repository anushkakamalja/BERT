# BERT
BERT Experimentation with Language Models

As I delve deeper into the fascinating world of Language Models (LMs), I find myself captivated by the capabilities of BERT (Bidirectional Encoder Representations from Transformers). In this repository, you'll find a Google Colab notebook where I've embarked on a journey of experimentation with BERT.

Through this notebook, I've explored various aspects of BERT, experimenting with its pre-trained model: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4  
for specific tasks. While still in the early stages of my learning journey, I've found the experience enriching and enlightening.

This project is part of my ongoing efforts to deepen my understanding of Language Models and Natural Language Processing (NLP) as a whole. As I continue to learn and explore, I plan to add more projects to this repository, each aimed at uncovering new insights and pushing the boundaries of my knowledge in this field.
